{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3  - Part B\n",
    "\n",
    "\n",
    "###### Roll Number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start Part B by taking out a minute to thank creators of Tensorflow and Keras for making our lives easier. The goal of this home work is to design a convolutional neural network to classify images from CIFAR-10. CIFAR-10 is a subset of CIFAR-100 with 10 and 100 being number of classes. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. We'll be building small CNNs using Keras to solve this classification task.\n",
    "<br>\n",
    "\n",
    "Please note:\n",
    "- You have to save each trained model weights and tensorboard logs in different folder. Use proper names for the model\n",
    "- You have to show trainings in this notebook file, so make sure that the printings and figures are available on this notebook file\n",
    "- This notebook is also your report file. So add your comments and analysis of each task. Your comments and analysis are as important as your code. This would be considered short report and it is compulsory part of the homework.\n",
    "- Your system memory may overflow. In that case, please reset memory for trained model(s) keeping printed logs and history of trained model. History of all trained models could be used to plot the trends.\n",
    "- For trends you shall also use tensorboard logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau, TensorBoard\n",
    "from keras.utils import plot_model, to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "your_id = 20100286\n",
    "np.random.seed(your_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "class_labels = np.arange(0, num_classes, 1)\n",
    "input_shape_cifar = (32,32,3)\n",
    "images_dir = './images/cifar/'\n",
    "epochs = 25\n",
    "all_histories = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1   Load dataset information\n",
    "Download the [CIFAR-10 Dataset](https://drive.google.com/drive/folders/1FPsts3C2tJMVQ1xcCi2XZdb2z_J2OjSr?usp=sharing) and setup your working directory. Be sure to put the dataset in the <code>images_dir</code> directory. We won't be loading images and will only store the file paths in memory. These file paths will be used to load and return batch of images in our generator. Feel free to explore the dataset in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = images_dir + 'train/'\n",
    "test_dir = images_dir + 'test/'\n",
    "label_dir = images_dir + 'labels.txt'\n",
    "\n",
    "# Use either os.listdir or glob to generate an array of file paths for training and testing data (I would recommend using glob)\n",
    "train_files = None\n",
    "test_files = None\n",
    "\n",
    "# Use labels.txt to store labels in an array/dict (NOTE: The line number corresponds to the actual label)\n",
    "classes = None\n",
    "\n",
    "# Use sci-kit learn's train_test_split method to generate a validation dataset from the training dataset\n",
    "val_files = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Preprocessing\n",
    "Preprocessing of data such as normalization, mean shift, make the learning task simple for network and could accelerate the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task 1.2.1 \n",
    "In images, pixel values range from 0 to 255. To shift the values between (0,1) range, divide input image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_norm(images):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task 1.2.2\n",
    "Compute the channel-wise mean and standard deviation (std). Subtract channel-wise mean and divide by channel-wise std. This will shift the mean to zero and variation to 1 for each channel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_meanShift(images):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3  Batch Generator\n",
    "Previously, when training our models, we were loading the complete data in memory to fit our model. However, in practice we're working with very large datasets which cannot be loaded all at once in memory. As a solution, we use \"Data Generators\" which are essentially python generators that load batches of data from disk into memory and pass into our models. In order to achieve this, we only store filepaths that point to training/test samples in our dataset in memory. A data generator yields a tuple of (Xs,Ys) whenever the generator is used via the next() function. Implementation of a data generator is open ended, you can add whatever arguments you find necessary in addition to the ones provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator( _ , batch_size = 32, preprocessing = None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4 Design CNN model\n",
    "**IMPORTANT**: For each model you create, you need to conduct analysis in a markdown cell below the code cell containing the model's training. Your analysis should include plots, comments on its accuracy/loss, etc. Try to predict images and see what images your model is correctly classifying, see if your model is overfitting or not. For each layer you add, specify its function and then comment on its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a CNN to predict classes for the images in the dataset. Try to keep your network shallow since you'll be training your model on your PC as well.\n",
    "input_img = Input(shape=(), name='input_img')\n",
    "model_initial = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'base_model'\n",
    "\n",
    "if not os.path.exists('./'+model_name):\n",
    "    os.mkdir(model_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # You may tweak the learning rate\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "# Use the appropriate loss function and compile the model\n",
    "model_initial.compile(loss=None, \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_initial, model_name+'/'+model_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_initial.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.5 Training model\n",
    "In order to train the model, instead of \"model.fit\" function, we will use the \"model.fit_generator\" function for training. For details check out [keras documentation](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRTensorBoard(TensorBoard):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(LRTensorBoard, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs.update({'lr': K.eval(self.model.optimizer.lr)})\n",
    "        super(LRTensorBoard, self).on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following callbacks to your Model: ModelCheckpoin, CSVLogger, ReduceLROnPlateau. The callback for Tensorboard has already been provided to you.\n",
    "checkpoint = None\n",
    "csvlogger = None\n",
    "reducelr = None\n",
    "tensorboard = LRTensorBoard(log_dir='./'+model_name, histogram_freq=0, write_graph=True, write_grads=1, \n",
    "                            batch_size=batch_size, write_images=True)\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.5.1 \n",
    "Train model without using any pre-processing scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three generators: For training your model, for validation, for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training your model here\n",
    "hist_initial = None\n",
    "\n",
    "all_histories.append(hist_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.5.2 \n",
    "Train model by using data preprocessing i.e normalization or mean shift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(input_shape), name='input_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_preprocessed_input'\n",
    "\n",
    "if not os.path.exists('./'+model_name):\n",
    "    os.mkdir(model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may tweak this Learning Rate\n",
    "adam = Adam(lr=0.001)\n",
    "\n",
    "# Compile your model using an appropriate loss\n",
    "model_preprocessed.compile(loss=None, \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_preprocessed, model_name+'/'+model_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preprocessed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following callbacks: ModelCheckpoint, CSVLogger. The callback for tensorboard has been provided to you.\n",
    "checkpoint = None\n",
    "csvlogger = None\n",
    "tensorboard = LRTensorBoard(log_dir='./'+model_name, histogram_freq=0, write_graph=True, write_grads=1, \n",
    "                            batch_size=batch_size, write_images=True)\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three generators again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_preprocessed = None\n",
    "all_histories.append(hist_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Pooling\n",
    "Now, you need to modify the model by adding pooling layer(s). Pooling could be average pooling or max pooling. You can use the size and stride for pooling of your choice. \n",
    "<br> \n",
    "Modify the network by introducing pooling layer and train the model using the __fit_generator__ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'model_pooling'\n",
    "\n",
    "input_img = Input(shape=(input_shape), name='input_img')\n",
    "model_pooled = None\n",
    "\n",
    "if not os.path.exists('./'+model_name):\n",
    "    os.mkdir(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "model_pooled.compile(loss=None, \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model_pooled, model_name+'/'+model_name+'.png')\n",
    "model_pooled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following callbacks: ModelCheckpoint, CSVLogger\n",
    "checkpoint = None\n",
    "csvlogger = None\n",
    "tensorboard = LRTensorBoard(log_dir='./'+model_name, histogram_freq=0, write_graph=True, write_grads=1, \n",
    "                            batch_size=batch_size, write_images=True)\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_pooled = None\n",
    "all_histories.append(hist_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 Dropout\n",
    "Modify the network by introducing dropout layer(s) and train the model using the __fit_generator__ function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_dropout'\n",
    "\n",
    "input_img = Input(shape=(input_shape), name='input_img')\n",
    "\n",
    "model_dropout = None\n",
    "\n",
    "if not os.path.exists('./'+model_name):\n",
    "    os.mkdir(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "model_dropout.compile(loss=None, \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_dropout, model_name+'/'+model_name+'.png')\n",
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "csvlogger = None\n",
    "tensorboard = LRTensorBoard(log_dir='./'+model_name, histogram_freq=0, write_graph=True, write_grads=1, \n",
    "                            batch_size=batch_size, write_images=True)\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_dropout = None\n",
    "\n",
    "all_histories.append(hist_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 Batch Normalization\n",
    "Modify the network by introducing batch normalization layer(s) and train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_batch_normalized'\n",
    "if not os.path.exists('./'+model_name):\n",
    "    os.mkdir(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(input_shape), name='input_img')\n",
    "\n",
    "model_normalized = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.001)\n",
    "model_normalized.compile(loss=None, \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_normalized, model_name+'/'+model_name+'.png')\n",
    "model_normalized.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the following checkpoints: ModelCheckpoint, CSVLogger\n",
    "checkpoint = None\n",
    "cvslogger = None\n",
    "tensorboard = LRTensorBoard(log_dir='./'+model_name, histogram_freq=0, write_graph=True, write_grads=1, \n",
    "                            batch_size=batch_size, write_images=True)\n",
    "\n",
    "callbacks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_normalized = None\n",
    "all_histories.append(hist_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1 Comparison of all models\n",
    "In this task you need to plot the loss and accuracy for all models and __discuss__ which model has better performance and why do you think so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy of all models\n",
    "def plotHistory(histories, names, metric):\n",
    "    plt.clf() \n",
    "    x_axis = list(range(epochs))\n",
    "    for hist, name in zip(histories, names):\n",
    "        plt.plot(x_axis, hist.history[metric], label=metric+'_'+name)\n",
    "        plt.scatter(x_axis, hist.history[metric])\n",
    "    plt.legend()\n",
    "    plt.title(metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histories of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard \n",
    "We will be covering Tensorboard in Friday's tutorial. You can complete this part after the tutorial (21st Feb 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run TensorBoard using the following command\n",
    "```tensorboard --logdir=path/to/log-directory\n",
    "```\n",
    "Now for logs of all above trained models, take snapshot of the _loss_ and _accuracy_ plots and save in the directory where this notebook is. To show that saved plot here in notebook use following command in _markdown_ cell. \n",
    "<br> __![](path_to_image)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add cells to include loss plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add your analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2 Visualize Activations\n",
    "We will be covering this in Friday's tutorial. You can complete this part after the tutorial (21st Feb 2020). In this task, you are required to visualize the activations (outputs) of both the convolution layers of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any image here\n",
    "test_image = None\n",
    "\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gca().grid(False);\n",
    "\n",
    "test_reshaped = test_image.reshape((1,32,32,3))\n",
    "\n",
    "# Get a prediction from ANY ONE of your models and output its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Model from the Input layer to a Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prediction for your chosen image using this model\n",
    "predicted_activations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your predictions to visualise the activations\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(predicted_activations[0,:,:,i]);\n",
    "    ax.grid(False);\n",
    "    ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another model to visualize activations from ANOTHER convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may rename this variable\n",
    "second_predicted_activations = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15,15), dpi=80)\n",
    "for i,ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(second_predicted_activations[0,:,:,i]);\n",
    "    ax.grid(False);\n",
    "    ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q) What features from your chosen image do you think your model has been able to learn? In terms of filters (weights), what goes on when we add more convolutional layers in our models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task 6.1  Effect of learning rate\n",
    "Take the best selected model in __Task 5.1__ and change its learning rate to: \n",
    "- 6.1.1 any value between 5 and 1.\n",
    "- 6.1.2 learning rate = any value between 0.6 and  0.1 \n",
    "- 6.1.3 learning rate = 0.00001\n",
    "\n",
    "and train these three models to compare the effect learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use code provided in previous tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2 Plot loss and accuracy\n",
    "Plot the loss and accracy for best model selected in task 5.1, and model trained in 6.1.1, 6.1.2 and 6.1.3 task. Give the detailed analysis of the effect of learning rate in your words and select the best learning rate for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plotHistory to make these plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
