{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 437 - Deep Learning - PA1:Neural Network Class\n",
    "\n",
    "*__Submission Instructions:__*\n",
    "- Rename this notebook to `hw1_rollnumber.ipynb` before submission on LMS.\n",
    "- All code must be written in this notebook (you do not need to submit any other files).\n",
    "- The output of all cells must be present in the version of the notebook you submit. You will be penalized if the output is absent.\n",
    "- The university honor code should be maintained. Any violation, if found, will result in disciplinary action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T14:39:17.671980Z",
     "start_time": "2019-01-30T14:38:40.137027Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from IPython.display import Image\n",
    "import pydot\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from class_tests import test_neural_network\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please write your roll number in the next cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T14:36:37.383767Z",
     "start_time": "2019-01-30T14:36:37.333537Z"
    }
   },
   "outputs": [],
   "source": [
    "rollnumber = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be creating 4 versions of the `NeuralNetwork` class. You will start with a simple 2 layer feed forward network and progressively modify the class by adding features to make it more generic. At the end you will have implemented a version which can create networks of arbitrary shape and depth, and which will work for both regression and classification tasks. \n",
    "\n",
    "Often in pratical situations, raw machine learning architecture and code is hidden behind libraries and simplfied toolkits. The purpose of this assignment is to lift that curtain and get you hands-on exprience working with the mathematical fundamentals of neural network architectures. After this, you'll know exactly how a network leverages 'gradient descent' to find optimal solutions and how forward and backward passes are implemented mathematically and in code.\n",
    "\n",
    "\n",
    "### Primary Task\n",
    "Skeleton code is provided to get you started; the main methods you need to implement correspond to the 3 steps of the training process, namely:\n",
    "1. Initialize variables and initialize weights\n",
    "2. Forward pass\n",
    "3. Backward pass AKA Backpropogation\n",
    "4. Weight Update AKA Gradient Descent\n",
    "\n",
    "__Look for comments in the code to see where you are supposed to write code.__ Essentially, you will be working on 5 functions. \n",
    "You should use the lecture [slides]() as reference for the equations. \n",
    "\n",
    "A `fit` function is what combines the previous three functions and overall trains the network to __fit__ to the provided training examples. In all the following tasks, the provided `fit` methods require the three steps of the training process to be correctly working. The function has been setup in a way that it expects the above 3 methods to take particular inputs and return particular outputs. __You are supposed to work within this restriction.__ Modification of the provided code without prior discussion with the TAs will result in a __grade deduction__. \n",
    "\n",
    "### Support code\n",
    "The class comes with a few useful methods for the activation functions, plotting and model error evaluation. Here are some points to note:\n",
    "1. Test cases have been provided for the functions (methods) you need to implement. \n",
    "2. These test cases are present in a separate python script but you will only need to use a function imported into this notebook to leverage them (usage is shown in each task). \n",
    "3. The test cases will check the 3 individual functions you will have written and but not the overall model that they fit into. \n",
    "4. The test cases expect you to implement stochastic gradient descent (and __not__ the other two implementations).\n",
    "5. The test cases are __not__ exhaustive so they do not guarantee that your solution is 100% correct. For that you should always look at the loss plot during training.\n",
    "\n",
    "To see how well your model is doing, you need to look at the dummy tasks (at the end) and make sure your model loss is going down during training. A dummy regression task of adding two numbers (sum less than 1) has been provided as well. Similarly, a dummy classification task (XOR logic gate) is also present. You can look at the shapes of the inputs and outputs matrices as well as the training trend (once you implement a full task) by using your own class (make sure you are using the correct arguments to the `__init__` method). \n",
    "\n",
    "You can find a demonstration of the neural network working on a synthetic dataset for both regression and classification at the end of the notebook. After you implement your class fully, you can play with the parameters and see the visualization change, we highly recommend that you try this. This part of the notebook will not be graded in any way, but it might give you a better insight/intuition into how the model makes decisions, and how important parameters are in terms of the usefullness of neural networks. You will explore the parameter space more thoroughly in the next assignment :P\n",
    "\n",
    "### Side note\n",
    "*The `plot_model` method will only work if you have the `pydot` python package installed along with [Graphviz](https://graphviz.gitlab.io/download/)). If you do not wish to use this then simply comment out the import for `pydot`.*\n",
    "\n",
    "### Need Help?\n",
    "If you need help, refer to your textbook (provided on LMS) which has examples and explanations for all the processes you'll have to implement, as well as rich details on functions such as `sigmoid` and `softmax`. Going over the book once before getting started is a good idea, you can also refer to the class slides and supplemental material provided with the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this task you will implement the simplest version of a feed forward neural network - a 2 layer network. \n",
    "\n",
    "Your code will only be partially vectorized, this means that you will be passing a single data point through the network at a time. In simple terms, the running time of your `fit` method will be $O(e*n)$ where $e$ is the number of epochs and $n$ is the number of data points (assuming all functions/methods called in `fit` take constant time). \n",
    "\n",
    "This version of the network will be using the `softmax` activation function for the output layer and `sigmoid` for the hidden layer, *ie.* a classification model which learns to output the joint probability mass function of the classes in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:25:17.044301Z",
     "start_time": "2019-01-27T01:25:16.984346Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    ## YOUR WORK STARTS HERE, YOU NEED TO LEAVE ALL THE OTHER FUNCTIONS AS THEY ARE\n",
    "    \n",
    "    def __init__(self, input_size, hidden_nodes, output_size):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        The parameters represent the number of nodes in each layer (total 3). \n",
    "        Look at the inputs to the function'''\n",
    "        self.num_layers = None # includes input layer\n",
    "        self.input_shape = None\n",
    "        self.hidden_shape = None\n",
    "        self.output_shape = None\n",
    "        \n",
    "        self.weights_ = self.biases_ = []\n",
    "        self.__init_weights()\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        W_h = np.random.normal(size=(None, None))\n",
    "        b_h = np.zeros(shape=(None,))\n",
    "        \n",
    "        W_o = np.random.normal(size=(None, None))\n",
    "        b_o = np.zeros(shape=(None,))\n",
    "        \n",
    "        self.weights_ = None\n",
    "        self.biases_  = None\n",
    "    \n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "        \n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''        \n",
    "        return deltas\n",
    "\n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "        \n",
    "        \n",
    "    ## YOUR WORK ENDS HERE, LEAVE ALL FOLLOWING FUNCTIONS ALONE\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            num_samples = Xs.shape[0]\n",
    "            for i in range(num_samples):\n",
    "                sample_input = Xs[i,:].reshape((1,self.input_shape))\n",
    "                sample_target = Ys[i,:].reshape((1,self.output_shape))\n",
    "                \n",
    "                activations = self.forward_pass(sample_input)\n",
    "                deltas = self.backward_pass(sample_target, activations)\n",
    "\n",
    "                layer_inputs = [sample_input] + activations[:-1]\n",
    "                self.weight_update(deltas, layer_inputs, lr)\n",
    "            \n",
    "            preds = self.predict(Xs)\n",
    "            current_loss = self.cross_entropy_loss(preds, Ys)\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "    \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        predictions = []\n",
    "        num_samples = Xs.shape[0]\n",
    "        for i in range(num_samples):\n",
    "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
    "            sample_prediction = self.forward_pass(sample)[-1]\n",
    "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(nodes_per_layer[i]):\n",
    "                for n2 in range(nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:25:23.531625Z",
     "start_time": "2019-01-27T01:25:20.453931Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(input_size=2, hidden_nodes=5, output_size=2)\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:25:30.833998Z",
     "start_time": "2019-01-27T01:25:30.643964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_neural_network(NeuralNetwork, 'task1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you will modify the class to allow the option to learn a regression model. You need to change some methods to account for the `mode` of the network. You can copy your code from task 1 as a starting point if you want.\n",
    "\n",
    "If the `mode` is classification, you will use your code from Task1. In case of the `mode` being regression you will apply the `sigmoid` activation function to both layers and compute the deltas for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:30:16.547173Z",
     "start_time": "2019-01-27T01:30:16.441918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    ## YOUR WORK STARTS HERE, YOU NEED TO LEAVE ALL THE OTHER FUNCTIONS AS THEY ARE\n",
    "    \n",
    "    def __init__(self, input_size, hidden_nodes, output_size, mode):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"mode\" can be one of 'regression' or 'classification' and controls the output activation as well as training metric\n",
    "        The rest of the parameters represent the number of nodes in each layer (total 3).'''\n",
    "        if mode not in ['classification','regression']:\n",
    "            raise ValueError('Only \"classification\" and \"regression\" modes are supported.')\n",
    "        \n",
    "        self.num_layers = None # includes input layer\n",
    "        self.input_shape = None\n",
    "        self.hidden_shape = None\n",
    "        self.output_shape = None\n",
    "        self.mode = None\n",
    "        \n",
    "        \n",
    "        self.weights_ = self.biases_ = []\n",
    "        self.__init_weights()\n",
    "    \n",
    "    def __init_weights(self):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        W_h = np.random.normal(size=(None, None))\n",
    "        b_h = np.zeros(shape=(None,))\n",
    "        \n",
    "        W_o = np.random.normal(size=(None, None))\n",
    "        b_o = np.zeros(shape=(None,))\n",
    "        \n",
    "        self.weights_ = None\n",
    "        self.biases_  = None\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "        \n",
    "            \n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "        \n",
    "        \n",
    "        return deltas\n",
    "    \n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "        \n",
    "    ## YOUR WORK ENDS HERE, LEAVE ALL FOLLOWING FUNCTIONS ALONE\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        print(Xs.shape)\n",
    "        print(Ys.shape)\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            num_samples = Xs.shape[0]\n",
    "            for i in range(num_samples):\n",
    "                sample_input = Xs[i,:].reshape((1,self.input_shape))\n",
    "                sample_target = Ys[i,:].reshape((1,self.output_shape))\n",
    "                \n",
    "                activations = self.forward_pass(sample_input)\n",
    "                deltas = self.backward_pass(sample_target, activations)\n",
    "\n",
    "                layer_inputs = [sample_input] + activations[:-1]\n",
    "                self.weight_update(deltas, layer_inputs, lr)\n",
    "            \n",
    "            preds = self.predict(Xs)\n",
    "            if self.mode == 'regression':\n",
    "                current_loss = self.mean_squared_error(preds, Ys)\n",
    "            elif self.mode == 'classification':\n",
    "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "    \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        predictions = []\n",
    "        num_samples = Xs.shape[0]\n",
    "        for i in range(num_samples):\n",
    "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
    "            sample_prediction = self.forward_pass(sample)[-1]\n",
    "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        if self.mode == 'regression':\n",
    "            return self.mean_squared_error(pred, Ys)\n",
    "        elif self.mode == 'classification':\n",
    "            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(nodes_per_layer[i]):\n",
    "                for n2 in range(nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(input_size=2, hidden_nodes=5, output_size=1, mode='regression')\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_neural_network(NeuralNetwork, 'task2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, if it helps copy only your code from Task2 as a starting point in the next cell. Now you will modify the class to allow an arbitrarily shaped network. You will need to change all 3 primary methods.\n",
    "\n",
    "*Hint: The output/last layer is special in terms of the delta calculation. All the hidden layers have the same calculation (chain rule)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:30:16.547173Z",
     "start_time": "2019-01-27T01:30:16.441918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    ## YOUR WORK STARTS HERE, YOU NEED TO LEAVE ALL THE OTHER FUNCTIONS AS THEY ARE   \n",
    "    \n",
    "    def __init__(self, nodes_per_layer, mode):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing number of nodes in each layer (including input layer)\n",
    "        \"mode\" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have atleast 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('Number of nodes in all layers must be positive.')\n",
    "        if mode not in ['classification','regression']:\n",
    "            raise ValueError('Only \"classification\" and \"regression\" modes are supported.')\n",
    "        \n",
    "        self.num_layers = None # includes input layer\n",
    "        self.nodes_per_layer = None\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        self.mode = None\n",
    "        \n",
    "        \n",
    "        self.__init_weights(nodes_per_layer)\n",
    "    \n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for i,_ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip input layer, it does not have weights/bias\n",
    "                continue\n",
    "            \n",
    "            weight_matrix = np.random.normal(size=(None, None))\n",
    "            self.weights_.append(None)\n",
    "            bias_vector = np.zeros(shape=(None,))\n",
    "            self.biases_.append(None)\n",
    "    \n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "                \n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "\n",
    "        return deltas\n",
    "    \n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "\n",
    "    ## YOUR WORK ENDS HERE, LEAVE ALL FOLLOWING FUNCTIONS ALONE\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        print(Xs.shape)\n",
    "        print(Ys.shape)\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            num_samples = Xs.shape[0]\n",
    "            for i in range(num_samples):\n",
    "                sample_input = Xs[i,:].reshape((1,self.input_shape))\n",
    "                sample_target = Ys[i,:].reshape((1,self.output_shape))\n",
    "                \n",
    "                activations = self.forward_pass(sample_input)\n",
    "                deltas = self.backward_pass(sample_target, activations)\n",
    "\n",
    "                layer_inputs = [sample_input] + activations[:-1]\n",
    "                self.weight_update(deltas, layer_inputs, lr)\n",
    "            \n",
    "            preds = self.predict(Xs)\n",
    "            if self.mode == 'regression':\n",
    "                current_loss = self.mean_squared_error(preds, Ys)\n",
    "            elif self.mode == 'classification':\n",
    "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "    \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        predictions = []\n",
    "        num_samples = Xs.shape[0]\n",
    "        for i in range(num_samples):\n",
    "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
    "            sample_prediction = self.forward_pass(sample)[-1]\n",
    "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        if self.mode == 'regression':\n",
    "            return self.mean_squared_error(pred, Ys)\n",
    "        elif self.mode == 'classification':\n",
    "            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(self.nodes_per_layer[i]):\n",
    "                for n2 in range(self.nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,5,2], 'classification')\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_neural_network(NeuralNetwork, 'task3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (Grads only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should copy your code from task3 as a starting point. You have to modify the forward and backward passes to use a different activation function; the Rectified Linear Unit (ReLu). You can look at the research papers or articles online for derivatives and other explanations. \n",
    "\n",
    "In this task you need to implement the activation function and its derivative yourself. You can use a separate function for derivative of activation function or include the expression in the derivative calculation. \n",
    "\n",
    "No automatic test cases are included for this task. You should use the dummy tasks and look at the trend of the loss plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_der(x):\n",
    "        return None\n",
    "    \n",
    "    ## YOUR WORK STARTS HERE, YOU NEED TO LEAVE ALL THE OTHER FUNCTIONS AS THEY ARE\n",
    "\n",
    "    def __init__(self, nodes_per_layer, mode):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing number of nodes in each layer (including input layer)\n",
    "        \"mode\" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have atleast 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('Number of nodes in all layers must be positive.')\n",
    "        if mode not in ['classification','regression']:\n",
    "            raise ValueError('Only \"classification\" and \"regression\" modes are supported.')\n",
    "        \n",
    "        self.num_layers = None # includes input layer\n",
    "        self.nodes_per_layer = None\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        self.mode = None\n",
    "        \n",
    "        \n",
    "        self.__init_weights(nodes_per_layer)\n",
    "    \n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for i,_ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip input layer, it does not have weights/bias\n",
    "                continue\n",
    "            \n",
    "            weight_matrix = np.random.normal(size=(None, None))\n",
    "            self.weights_.append(None)\n",
    "            bias_vector = np.zeros(shape=(None,))\n",
    "            self.biases_.append(None)\n",
    "\n",
    "    \n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "    \n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "        \n",
    "        return deltas\n",
    "\n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "            \n",
    "    ## YOUR WORK ENDS HERE, LEAVE ALL FOLLOWING FUNCTIONS ALONE\n",
    "    \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            num_samples = Xs.shape[0]\n",
    "\n",
    "            activations = self.forward_pass(Xs)\n",
    "            deltas = self.backward_pass(Ys, activations)\n",
    "\n",
    "            layer_inputs = [Xs] + activations[:-1]\n",
    "            self.weight_update(deltas, layer_inputs, lr)\n",
    "            \n",
    "            preds = self.predict(Xs)\n",
    "            if self.mode == 'regression':\n",
    "                current_loss = self.mean_squared_error(preds, Ys)\n",
    "            elif self.mode == 'classification':\n",
    "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "    \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        predictions = []\n",
    "        num_samples = Xs.shape[0]\n",
    "        for i in range(num_samples):\n",
    "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
    "            sample_prediction = self.forward_pass(sample)[-1]\n",
    "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        if self.mode == 'regression':\n",
    "            return self.mean_squared_error(pred, Ys)\n",
    "        elif self.mode == 'classification':\n",
    "            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(self.nodes_per_layer[i]):\n",
    "                for n2 in range(self.nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,5,2], 'classification')\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neural_network(NeuralNetwork, 'task4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, if it helps copy only your code from Task3 as a starting point in the next cell. To cap off this assignment you will fully vectorize your implementation. This means changing the primary functions again. There will be a handout on LMS to explain this further.\n",
    "\n",
    "After you do this, the runtime of the `fit` function will just be $O(e)$ (again, assuming all functions called in the loop take constant time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:30:16.547173Z",
     "start_time": "2019-01-27T01:30:16.441918Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_loss(y_pred, y_true):\n",
    "        return -(y_true * np.log(y_pred)).sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        return np.sum(y_pred == y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        expx = np.exp(x)\n",
    "        return expx / expx.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    ## YOUR WORK STARTS HERE, YOU NEED TO LEAVE ALL THE OTHER FUNCTIONS AS THEY ARE\n",
    "    \n",
    "    def __init__(self, nodes_per_layer, mode):\n",
    "        '''Creates a Feed-Forward Neural Network.\n",
    "        \"nodes_per_layer\" is a list containing number of nodes in each layer (including input layer)\n",
    "        \"mode\" can be one of 'regression' or 'classification' and controls the output activation as well as training metric'''\n",
    "        if len(nodes_per_layer) < 2:\n",
    "            raise ValueError('Network must have atleast 2 layers (input and output).')\n",
    "        if not (np.array(nodes_per_layer) > 0).all():\n",
    "            raise ValueError('Number of nodes in all layers must be positive.')\n",
    "        if mode not in ['classification','regression']:\n",
    "            raise ValueError('Only \"classification\" and \"regression\" modes are supported.')\n",
    "        \n",
    "        self.num_layers = None # includes input layer\n",
    "        self.nodes_per_layer = None\n",
    "        self.input_shape = None\n",
    "        self.output_shape = None\n",
    "        self.mode = None\n",
    "        \n",
    "        self.__init_weights(nodes_per_layer)\n",
    "    \n",
    "    def __init_weights(self, nodes_per_layer):\n",
    "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
    "        self.weights_ = []\n",
    "        self.biases_ = []\n",
    "        for i,_ in enumerate(nodes_per_layer):\n",
    "            if i == 0:\n",
    "                # skip input layer, it does not have weights/bias\n",
    "                continue\n",
    "            \n",
    "            weight_matrix = np.random.normal(size=(None, None))\n",
    "            self.weights_.append(None)\n",
    "            bias_vector = np.zeros(shape=(None,))\n",
    "            self.biases_.append(None)\n",
    "    \n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''Executes the feed forward algorithm.\n",
    "        \"input_data\" is the input to the network in row-major form\n",
    "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
    "\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, targets, layer_activations):\n",
    "        '''Executes the backpropogation algorithm.\n",
    "        \"targets\" is the ground truth/labels\n",
    "        \"layer_activations\" are the return value of the forward pass step\n",
    "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
    "\n",
    "\n",
    "        return deltas\n",
    "\n",
    "    def weight_update(self, deltas, layer_inputs, lr):\n",
    "        '''Executes the gradient descent algorithm.\n",
    "        \"deltas\" is return value of the backward pass step\n",
    "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
    "        \"lr\" is the learning rate'''\n",
    "            \n",
    "\n",
    "    ## YOUR WORK ENDS HERE, LEAVE ALL FOLLOWING FUNCTIONS ALONE\n",
    "    \n",
    "        \n",
    "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
    "        '''Trains the model on the given dataset for \"epoch\" number of itterations with step size=\"lr\". \n",
    "        Returns list containing loss for each epoch.'''\n",
    "        history = []\n",
    "        for epoch in tqdm_notebook(range(epochs)):\n",
    "            num_samples = Xs.shape[0]\n",
    "            sample_input = Xs\n",
    "            sample_target = Ys\n",
    "\n",
    "            activations = self.forward_pass(sample_input)\n",
    "            deltas = self.backward_pass(sample_target, activations)\n",
    "\n",
    "            layer_inputs = [sample_input] + activations[:-1]\n",
    "            self.weight_update(deltas, layer_inputs, lr)\n",
    "            \n",
    "            preds = self.predict(Xs)\n",
    "            if self.mode == 'regression':\n",
    "                current_loss = self.mean_squared_error(preds, Ys)\n",
    "            elif self.mode == 'classification':\n",
    "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
    "            history.append(current_loss)\n",
    "        return history\n",
    "        \n",
    "    def predict(self, Xs):\n",
    "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
    "        return self.forward_pass(Xs)[-1]\n",
    "    \n",
    "    def evaluate(self, Xs, Ys):\n",
    "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
    "        pred = self.predict(Xs)\n",
    "        if self.mode == 'regression':\n",
    "            return self.mean_squared_error(pred, Ys)\n",
    "        elif self.mode == 'classification':\n",
    "            return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
    "    \n",
    "    def plot_model(self, filename):\n",
    "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
    "        graph = pydot.Dot(graph_type='digraph')\n",
    "        graph.set_rankdir('LR')\n",
    "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
    "        for i in range(self.num_layers-1):\n",
    "            for n1 in range(self.nodes_per_layer[i]):\n",
    "                for n2 in range(self.nodes_per_layer[i+1]):\n",
    "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
    "                    graph.add_edge(edge)\n",
    "        graph.write_png(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,5,2], 'classification')\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_neural_network(NeuralNetwork, 'task5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:33:04.290893Z",
     "start_time": "2019-01-27T01:33:03.930796Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.uniform(low=0.0, high=0.5, size=(150,))\n",
    "b = np.random.uniform(low=0.0, high=0.5, size=(150,))\n",
    "dataset = pd.DataFrame({\n",
    "    'var1':   a,\n",
    "    'var2':   b,\n",
    "    'output': a+b,\n",
    "})\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:33:44.580767Z",
     "start_time": "2019-01-27T01:33:44.565440Z"
    }
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,3,5,1], 'regression')\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-30T14:39:43.311192Z",
     "start_time": "2019-01-30T14:39:41.481315Z"
    }
   },
   "outputs": [],
   "source": [
    "history = nn.fit(dataset[['var1','var2']].values, dataset[['output']].values, epochs=2000, lr=0.001)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:33:49.920877Z",
     "start_time": "2019-01-27T01:33:49.860922Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = np.array([[0.4,0.1],\n",
    "                      [0.2,0.3]])\n",
    "nn.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:33:56.370640Z",
     "start_time": "2019-01-27T01:33:56.320595Z"
    }
   },
   "outputs": [],
   "source": [
    "# XOR logic operator\n",
    "dataset = pd.DataFrame({\n",
    "    'var1':   [0, 0, 1, 1],\n",
    "    'var2':   [0, 1, 0, 1],\n",
    "    'output': [0, 1, 1, 0],\n",
    "})\n",
    "dataset = pd.get_dummies(dataset, columns=['output'])\n",
    "dataset['output'] = pd.Series([0, 1, 1, 0])\n",
    "print(dataset.shape)\n",
    "dataset.head()\n",
    "# The columns 'output_0' and 'output_1' are one-hot encoded representation of the categorical column 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:33:57.600944Z",
     "start_time": "2019-01-27T01:33:57.580951Z"
    }
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,5,2], 'classification')\n",
    "nn.plot_model('graph.png')\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:01.160593Z",
     "start_time": "2019-01-27T01:33:59.950512Z"
    }
   },
   "outputs": [],
   "source": [
    "history = nn.fit(dataset[['var1','var2']].values, dataset[['output_0','output_1']].values, epochs=3000, lr=0.01)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:03.210522Z",
     "start_time": "2019-01-27T01:34:03.190866Z"
    }
   },
   "outputs": [],
   "source": [
    "nn.predict(dataset[['var1','var2']].values).argmax(axis=1) == dataset[['output_0','output_1']].values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Demos adapted from tutorial 1, refer to it if you need a refresher (available on LMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:17.130755Z",
     "start_time": "2019-01-27T01:34:16.900688Z"
    }
   },
   "outputs": [],
   "source": [
    "data_x, _ = make_moons(200, noise=0.18)\n",
    "plt.scatter(data_x[:,0], data_x[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:20.010456Z",
     "start_time": "2019-01-27T01:34:19.400302Z"
    }
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg = reg.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));\n",
    "\n",
    "d = np.arange(-1.5, 2.5, 0.1).reshape((40,1))\n",
    "preds = reg.predict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:22.270599Z",
     "start_time": "2019-01-27T01:34:21.920681Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));\n",
    "ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');\n",
    "ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Linear Regression {}'.format(rollnumber));\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:56.080312Z",
     "start_time": "2019-01-27T01:34:23.960235Z"
    }
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([1,10,20,10,1], 'regression')\n",
    "history = nn.fit(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)), epochs=20000, lr=1e-4)\n",
    "preds = nn.predict(d)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='MSE', title='Training Plot {}'.format(rollnumber));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:34:58.560476Z",
     "start_time": "2019-01-27T01:34:58.265273Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data_x[:,0].reshape((200,1)), data_x[:,1].reshape((200,1)));\n",
    "ax.plot(d.flatten(), preds.flatten(), c='tab:red', label='Prediction');\n",
    "ax.set(xlabel='Input Feature', ylabel='Target Variable', title='Neural Network Regression {}'.format(rollnumber));\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Classification Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:05.630191Z",
     "start_time": "2019-01-27T01:35:05.600080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "# If you don't fully understand this function don't worry\n",
    "def plot_decision_boundary(pred_func, x_min, x_max, y_min, y_max, cmap, ax):\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.flatten(), yy.flatten()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:07.850117Z",
     "start_time": "2019-01-27T01:35:07.570257Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_x, data_y = make_moons(200, noise=0.20)\n",
    "plt.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "plt.gca().set(xlabel='Feature 1', ylabel='Feature 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:09.680260Z",
     "start_time": "2019-01-27T01:35:09.399995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf = clf.fit(data_x, data_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:11.610280Z",
     "start_time": "2019-01-27T01:35:10.910048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: clf.predict(x), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title='Logistic Regression Classifier {}'.format(rollnumber));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:18.080304Z",
     "start_time": "2019-01-27T01:35:13.530156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([2,10,10,2], 'classification')\n",
    "history = nn.fit(data_x, pd.get_dummies(data_y).values, epochs=5000, lr=1e-3)\n",
    "plt.plot(history);\n",
    "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-27T01:35:21.050129Z",
     "start_time": "2019-01-27T01:35:20.240234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x_min, x_max = data_x[:, 0].min() - .5, data_x[:, 0].max() + .5\n",
    "y_min, y_max = data_x[:, 1].min() - .5, data_x[:, 1].max() + .5\n",
    "plot_decision_boundary(lambda x: nn.predict(x).argmax(axis=1), \n",
    "                       x_min, x_max, y_min, y_max, \n",
    "                       plt.cm.Spectral, ax)\n",
    "ax.scatter(data_x[:,0], data_x[:,1], c=data_y, cmap=plt.cm.Spectral);\n",
    "ax.set(xlabel='Feature 1', ylabel='Feature 2', title='Neural Network Classifier {}'.format(rollnumber));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
